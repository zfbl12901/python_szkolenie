---
title: "Exploitation des LLM (Large Language Models)"
order: 21
parent: "20-ia-introduction.md"
tags: ["python", "ia", "llm", "openai", "anthropic", "langchain"]
---

# Exploitation des LLM (Large Language Models)

## Qu'est-ce qu'un LLM ?

Un **Large Language Model (LLM)** est un modèle d'IA entraîné sur d'énormes quantités de texte pour comprendre et générer du langage naturel. Les LLM modernes peuvent :

- Comprendre le contexte et les nuances
- Générer du texte cohérent et pertinent
- Répondre à des questions complexes
- Traduire entre langues
- Résumer de longs documents
- Coder et déboguer

## Les principaux LLM disponibles

### 1. OpenAI GPT (GPT-3.5, GPT-4)

- **GPT-3.5-turbo** : Rapide, économique, bon pour la plupart des cas
- **GPT-4** : Plus puissant, meilleur raisonnement, plus cher
- **GPT-4-turbo** : Version optimisée de GPT-4

### 2. Anthropic Claude

- **Claude 3 Opus** : Le plus puissant
- **Claude 3 Sonnet** : Équilibre performance/coût
- **Claude 3 Haiku** : Rapide et économique

### 3. Modèles open-source

- **Llama 2/3** (Meta) : Modèles open-source performants
- **Mistral** : Modèles français performants
- **Falcon** : Modèles open-source compétitifs

## Architecture de base d'un appel LLM

```python
# Structure générale d'un appel LLM
def appel_llm_base(prompt, model="gpt-3.5-turbo"):
    """
    Structure de base pour appeler un LLM
    
    Args:
        prompt: Le texte d'entrée
        model: Le modèle à utiliser
    
    Returns:
        La réponse du modèle
    """
    # 1. Préparer la requête
    messages = [
        {"role": "system", "content": "Tu es un assistant utile."},
        {"role": "user", "content": prompt}
    ]
    
    # 2. Appeler l'API
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.7,  # Créativité (0-2)
        max_tokens=500     # Longueur max de la réponse
    )
    
    # 3. Extraire la réponse
    return response.choices[0].message.content
```

## Concepts clés

### 1. Les rôles dans les messages

Les LLM modernes utilisent un système de rôles :

- **`system`** : Définit le comportement et la personnalité du modèle
- **`user`** : Messages de l'utilisateur
- **`assistant`** : Réponses précédentes du modèle (pour la conversation)

```python
messages = [
    {
        "role": "system",
        "content": "Tu es un expert Python qui explique les concepts simplement."
    },
    {
        "role": "user",
        "content": "Qu'est-ce qu'une fonction lambda ?"
    },
    {
        "role": "assistant",
        "content": "Une fonction lambda est une fonction anonyme..."
    },
    {
        "role": "user",
        "content": "Peux-tu me donner un exemple ?"
    }
]
```

### 2. Temperature et créativité

La **temperature** contrôle la créativité du modèle :

- **0.0 - 0.3** : Déterministe, prévisible (idéal pour tâches factuelles)
- **0.7 - 1.0** : Équilibré (bon pour la plupart des cas)
- **1.5 - 2.0** : Très créatif (pour la génération créative)

```python
# Exemple : Génération de code (basse température)
code_response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Écris une fonction Python pour trier une liste"}],
    temperature=0.2  # On veut du code précis, pas créatif
)

# Exemple : Génération créative (haute température)
creative_response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Écris une histoire courte sur un robot"}],
    temperature=1.2  # On veut de la créativité
)
```

### 3. Tokens et limites

Les **tokens** sont des unités de texte (environ 4 caractères en moyenne).

```python
# Vérifier le nombre de tokens
from tiktoken import encoding_for_model

encoding = encoding_for_model("gpt-3.5-turbo")
tokens = encoding.encode("Bonjour, comment allez-vous ?")
print(f"Nombre de tokens : {len(tokens)}")
# Output: Nombre de tokens : 7

# Limites importantes
# - GPT-3.5-turbo : 4096 tokens de contexte
# - GPT-4 : 8192 tokens de contexte
# - GPT-4-turbo : 128000 tokens de contexte
```

## Cas d'usage pratiques

### 1. Assistant de code

```python
def assistant_code(question_code, code_existant=""):
    """Assistant pour aider avec le code Python"""
    
    prompt = f"""
    Tu es un expert Python. Aide-moi avec cette question :
    
    Question: {question_code}
    
    Code existant:
    ```python
    {code_existant}
    ```
    
    Fournis une explication claire et un exemple de code si nécessaire.
    """
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3  # Basse température pour du code précis
    )
    
    return response.choices[0].message.content
```

### 2. Traducteur contextuel

```python
def traduire_avec_contexte(texte, langue_cible, contexte=""):
    """Traduction avec contexte pour meilleure précision"""
    
    prompt = f"""
    Traduis ce texte en {langue_cible}.
    
    Contexte: {contexte}
    
    Texte à traduire: {texte}
    
    Fournis une traduction naturelle et contextuellement appropriée.
    """
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5
    )
    
    return response.choices[0].message.content

# Exemple d'utilisation
traduction = traduire_avec_contexte(
    "I'll be back",
    "français",
    "Film de science-fiction, citation d'Arnold Schwarzenegger"
)
# Output: "Je reviendrai" (au lieu de "Je serai de retour")
```

### 3. Générateur de contenu

```python
def generer_article(titre, style="professionnel", longueur="moyen"):
    """Génère un article sur un sujet donné"""
    
    longueurs = {
        "court": 200,
        "moyen": 500,
        "long": 1000
    }
    
    prompt = f"""
    Écris un article sur le sujet suivant :
    
    Titre: {titre}
    Style: {style}
    Longueur: {longueurs[longueur]} mots environ
    
    L'article doit être bien structuré avec une introduction, un développement et une conclusion.
    """
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.8,
        max_tokens=longueurs[longueur] * 2  # Tokens ≈ mots * 1.3
    )
    
    return response.choices[0].message.content
```

### 4. Analyseur de sentiment

```python
def analyser_sentiment(texte):
    """Analyse le sentiment d'un texte"""
    
    prompt = f"""
    Analyse le sentiment de ce texte et classe-le dans une de ces catégories :
    - Positif
    - Négatif
    - Neutre
    
    Pour chaque catégorie, fournis aussi un score de confiance (0-100).
    
    Texte: {texte}
    
    Réponds au format JSON :
    {{
        "sentiment": "positif/negatif/neutre",
        "score": 85,
        "explication": "Explication courte"
    }}
    """
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        response_format={"type": "json_object"}  # Force la réponse en JSON
    )
    
    import json
    return json.loads(response.choices[0].message.content)
```

## Gestion des conversations

### Conversation simple

```python
class ConversationSimple:
    def __init__(self, system_prompt="Tu es un assistant utile."):
        self.messages = [
            {"role": "system", "content": system_prompt}
        ]
    
    def ajouter_message(self, role, content):
        self.messages.append({"role": role, "content": content})
    
    def obtenir_reponse(self, message_utilisateur):
        # Ajouter le message de l'utilisateur
        self.ajouter_message("user", message_utilisateur)
        
        # Obtenir la réponse
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=self.messages
        )
        
        reponse = response.choices[0].message.content
        
        # Ajouter la réponse à l'historique
        self.ajouter_message("assistant", reponse)
        
        return reponse

# Utilisation
chat = ConversationSimple("Tu es un expert en cuisine.")
print(chat.obtenir_reponse("Comment faire une tarte aux pommes ?"))
print(chat.obtenir_reponse("Et si je veux la rendre plus sucrée ?"))
```

### Conversation avec mémoire limitée

```python
class ConversationMemoire:
    def __init__(self, system_prompt, max_messages=10):
        self.system_prompt = system_prompt
        self.max_messages = max_messages
        self.messages = [{"role": "system", "content": system_prompt}]
    
    def obtenir_reponse(self, message_utilisateur):
        # Ajouter le message
        self.messages.append({"role": "user", "content": message_utilisateur})
        
        # Limiter la taille de l'historique
        if len(self.messages) > self.max_messages:
            # Garder le system prompt et les derniers messages
            self.messages = (
                [self.messages[0]] +  # System prompt
                self.messages[-(self.max_messages-1):]  # Derniers messages
            )
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=self.messages
        )
        
        reponse = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": reponse})
        
        return reponse
```

## Gestion des erreurs et retry

```python
import time
from openai import OpenAI, RateLimitError, APIError

def appel_llm_robuste(messages, max_retries=3, delay=1):
    """Appel LLM avec gestion d'erreurs et retry automatique"""
    
    client = OpenAI()
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages
            )
            return response.choices[0].message.content
            
        except RateLimitError:
            if attempt < max_retries - 1:
                wait_time = delay * (2 ** attempt)  # Backoff exponentiel
                print(f"Limite atteinte, attente de {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise Exception("Trop de tentatives, limite de taux dépassée")
                
        except APIError as e:
            print(f"Erreur API : {e}")
            if attempt < max_retries - 1:
                time.sleep(delay)
            else:
                raise
                
        except Exception as e:
            print(f"Erreur inattendue : {e}")
            raise
    
    return None
```

## Optimisation des coûts

### 1. Choisir le bon modèle

```python
# Tâche simple → Modèle économique
def tache_simple(texte):
    return client.chat.completions.create(
        model="gpt-3.5-turbo",  # Moins cher
        messages=[{"role": "user", "content": texte}]
    )

# Tâche complexe → Modèle puissant
def tache_complexe(texte):
    return client.chat.completions.create(
        model="gpt-4",  # Plus cher mais plus puissant
        messages=[{"role": "user", "content": texte}]
    )
```

### 2. Limiter les tokens

```python
def optimiser_prompt(texte, max_tokens=1000):
    """Tronque intelligemment le texte"""
    # Estimation : 1 token ≈ 4 caractères
    max_chars = max_tokens * 4
    
    if len(texte) > max_chars:
        # Tronquer en gardant le début et la fin
        debut = texte[:max_chars // 2]
        fin = texte[-max_chars // 2:]
        return f"{debut}...\n\n[Texte tronqué]\n\n...{fin}"
    
    return texte
```

### 3. Mise en cache des réponses

```python
from functools import lru_cache
import hashlib

def hash_prompt(prompt):
    """Crée un hash du prompt pour le cache"""
    return hashlib.md5(prompt.encode()).hexdigest()

cache = {}

def appel_llm_cache(prompt):
    """Appel LLM avec cache"""
    prompt_hash = hash_prompt(prompt)
    
    if prompt_hash in cache:
        print("Réponse depuis le cache")
        return cache[prompt_hash]
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    
    resultat = response.choices[0].message.content
    cache[prompt_hash] = resultat
    
    return resultat
```

## Bonnes pratiques

### ✅ À faire

- Utiliser des prompts clairs et spécifiques
- Gérer les erreurs et les limites de taux
- Optimiser les coûts en choisissant le bon modèle
- Tester différents paramètres (temperature, max_tokens)
- Valider et nettoyer les réponses

### ❌ À éviter

- Ne pas vérifier les réponses du modèle
- Ignorer la gestion d'erreurs
- Utiliser GPT-4 pour tout (coûteux)
- Oublier de limiter les tokens
- Hardcoder les clés API

## Prochaines étapes

Maintenant que vous maîtrisez les bases des LLM :

1. Explorez les **APIs spécifiques** (OpenAI, Anthropic, LangChain)
2. Apprenez les **Embeddings** pour la recherche sémantique
3. Découvrez **Qdrant** pour stocker des données vectorielles
4. Maîtrisez le **Prompt Engineering** pour optimiser vos interactions
